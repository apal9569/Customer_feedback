{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Customer_feedback.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apal9569/Customer_feedback/blob/master/Customer_feedback.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpQOOIiAKMKr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22VRKbEpKU_h",
        "colab_type": "code",
        "outputId": "66277a62-b205-4255-f6ff-a87d5b6a7ffc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzhsX6XVKVTu",
        "colab_type": "code",
        "outputId": "96ef268e-3f58-4b98-c5f3-5049e190131c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd drive/My Drive"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOxND6g8kkTN",
        "colab_type": "text"
      },
      "source": [
        "**IMPORTING NECESSARY LIBERARIES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCzbYRrVKos7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qV7hxE0RKwFS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train=pd.read_csv('Dataset/train.csv')\n",
        "test=pd.read_csv('Dataset/test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2SgZwmQK2-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqCy4CxUkvf4",
        "colab_type": "text"
      },
      "source": [
        "**separating the columns of the dataframe**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KrEMlY2K5qi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RevText_train=train['Review Text']\n",
        "RevTitle_train=train['Review Title']\n",
        "topic=train['topic']\n",
        "RevText_test=test['Review Text']\n",
        "RevTitle_test=test['Review Title']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKHmQmANk93X",
        "colab_type": "text"
      },
      "source": [
        "Taking the unique values in the topic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2byqLMD0r1jV",
        "colab_type": "code",
        "outputId": "19ee7c6a-73e8-42a2-ea70-23895ed2c177",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "source": [
        "r=list(topic.unique())\n",
        "r"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Shipment and delivery',\n",
              " 'Not Effective',\n",
              " 'Customer Service',\n",
              " 'Allergic',\n",
              " 'Texture',\n",
              " 'Quality/Contaminated',\n",
              " 'Color and texture',\n",
              " 'Bad Taste/Flavor',\n",
              " 'Too big to swallow',\n",
              " 'Smells Bad',\n",
              " 'Too Sweet',\n",
              " 'Ingredients',\n",
              " 'Expiry',\n",
              " 'Packaging',\n",
              " 'Wrong Product received',\n",
              " 'Pricing',\n",
              " 'False Advertisement',\n",
              " 'Inferior to competitors',\n",
              " \"Didn't Like\",\n",
              " 'Customer Issues',\n",
              " 'Hard to Chew']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHShoW0xlIOT",
        "colab_type": "text"
      },
      "source": [
        "Label Encoding the topic values "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41EiixzlrJ4d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labencod=LabelEncoder()\n",
        "re=labencod.fit_transform(r)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQcr8vINlS8P",
        "colab_type": "text"
      },
      "source": [
        "Creating a dictionary that contains for the topics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDg2aiak0flR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result_dict=dict()\n",
        "for key, value in zip(re,r):\n",
        "  result_dict[key]=value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGWDr1nl0_qj",
        "colab_type": "code",
        "outputId": "7a507499-68e1-4e34-f806-970855fa384c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "result_dict[12]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Packaging'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaAyQnBrLJ3S",
        "colab_type": "code",
        "outputId": "55518ce1-f0d1-48f3-f962-a6c912235492",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "print(RevText_train.head(1))\n",
        "\n",
        "print(topic.head(1))\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    Did nothing for me, didn't help lost even with...\n",
            "Name: Review Text, dtype: object\n",
            "0    Shipment and delivery\n",
            "Name: topic, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fBzXBtlljtq",
        "colab_type": "text"
      },
      "source": [
        "Function for converting the text into lower case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WPb3NAYNhSJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def lower_case(data):\n",
        "  Text=[]\n",
        "  \n",
        "  for i in range(len(data)):\n",
        "    Text.append(data[i].lower())\n",
        "  return Text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bpmv85HKasmO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RevText_train=lower_case(RevText_train)\n",
        "RevTitle_train=lower_case(RevTitle_train)\n",
        "RevText_test=lower_case(RevText_test)\n",
        "RevTitle_test=lower_case(RevTitle_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJIesMr-SGBr",
        "colab_type": "code",
        "outputId": "f7d90366-a944-48fe-eff7-f6fce18f487a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.stem import PorterStemmer "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UBirdjkmmtu",
        "colab_type": "text"
      },
      "source": [
        "**This whole module is to creating a random data from the existing one by interchanging, deleting, replacing with synonyms in the text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UO29JuOr8a-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "from random import shuffle\n",
        "random.seed(1)\n",
        "\n",
        "#stop words list\n",
        "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', \n",
        "\t\t\t'ours', 'ourselves', 'you', 'your', 'yours', \n",
        "\t\t\t'yourself', 'yourselves', 'he', 'him', 'his', \n",
        "\t\t\t'himself', 'she', 'her', 'hers', 'herself', \n",
        "\t\t\t'it', 'its', 'itself', 'they', 'them', 'their', \n",
        "\t\t\t'theirs', 'themselves', 'what', 'which', 'who', \n",
        "\t\t\t'whom', 'this', 'that', 'these', 'those', 'am', \n",
        "\t\t\t'is', 'are', 'was', 'were', 'be', 'been', 'being', \n",
        "\t\t\t'have', 'has', 'had', 'having', 'do', 'does', 'did',\n",
        "\t\t\t'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or',\n",
        "\t\t\t'because', 'as', 'until', 'while', 'of', 'at', \n",
        "\t\t\t'by', 'for', 'with', 'about', 'against', 'between',\n",
        "\t\t\t'into', 'through', 'during', 'before', 'after', \n",
        "\t\t\t'above', 'below', 'to', 'from', 'up', 'down', 'in',\n",
        "\t\t\t'out', 'on', 'off', 'over', 'under', 'again', \n",
        "\t\t\t'further', 'then', 'once', 'here', 'there', 'when', \n",
        "\t\t\t'where', 'why', 'how', 'all', 'any', 'both', 'each', \n",
        "\t\t\t'few', 'more', 'most', 'other', 'some', 'such', 'no', \n",
        "\t\t\t'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', \n",
        "\t\t\t'very', 's', 't', 'can', 'will', 'just', 'don', \n",
        "\t\t\t'should', 'now', '']\n",
        "\n",
        "#cleaning up text\n",
        "import re\n",
        "def get_only_chars(line):\n",
        "\n",
        "    clean_line = \"\"\n",
        "\n",
        "    line = line.replace(\"’\", \"\")\n",
        "    line = line.replace(\"'\", \"\")\n",
        "    line = line.replace(\"-\", \" \") #replace hyphens with spaces\n",
        "    line = line.replace(\"\\t\", \" \")\n",
        "    line = line.replace(\"\\n\", \" \")\n",
        "    line = line.lower()\n",
        "\n",
        "    for char in line:\n",
        "        if char in 'qwertyuiopasdfghjklzxcvbnm ':\n",
        "            clean_line += char\n",
        "        else:\n",
        "            clean_line += ' '\n",
        "\n",
        "    clean_line = re.sub(' +',' ',clean_line) #delete extra spaces\n",
        "    if clean_line[0] == ' ':\n",
        "        clean_line = clean_line[1:]\n",
        "    return clean_line\n",
        "\n",
        "########################################################################\n",
        "# Synonym replacement\n",
        "# Replace n words in the sentence with synonyms from wordnet\n",
        "########################################################################\n",
        "\n",
        "#for the first time you use wordnet\n",
        "#import nltk\n",
        "#nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet \n",
        "\n",
        "def synonym_replacement(words, n):\n",
        "\tnew_words = words.copy()\n",
        "\trandom_word_list = list(set([word for word in words if word not in stop_words]))\n",
        "\trandom.shuffle(random_word_list)\n",
        "\tnum_replaced = 0\n",
        "\tfor random_word in random_word_list:\n",
        "\t\tsynonyms = get_synonyms(random_word)\n",
        "\t\tif len(synonyms) >= 1:\n",
        "\t\t\tsynonym = random.choice(list(synonyms))\n",
        "\t\t\tnew_words = [synonym if word == random_word else word for word in new_words]\n",
        "\t\t\t#print(\"replaced\", random_word, \"with\", synonym)\n",
        "\t\t\tnum_replaced += 1\n",
        "\t\tif num_replaced >= n: #only replace up to n words\n",
        "\t\t\tbreak\n",
        "\n",
        "\t#this is stupid but we need it, trust me\n",
        "\tsentence = ' '.join(new_words)\n",
        "\tnew_words = sentence.split(' ')\n",
        "\n",
        "\treturn new_words\n",
        "\n",
        "def get_synonyms(word):\n",
        "\tsynonyms = set()\n",
        "\tfor syn in wordnet.synsets(word): \n",
        "\t\tfor l in syn.lemmas(): \n",
        "\t\t\tsynonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
        "\t\t\tsynonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n",
        "\t\t\tsynonyms.add(synonym) \n",
        "\tif word in synonyms:\n",
        "\t\tsynonyms.remove(word)\n",
        "\treturn list(synonyms)\n",
        "\n",
        "########################################################################\n",
        "# Random deletion\n",
        "# Randomly delete words from the sentence with probability p\n",
        "########################################################################\n",
        "\n",
        "def random_deletion(words, p):\n",
        "\n",
        "\t#obviously, if there's only one word, don't delete it\n",
        "\tif len(words) == 1:\n",
        "\t\treturn words\n",
        "\n",
        "\t#randomly delete words with probability p\n",
        "\tnew_words = []\n",
        "\tfor word in words:\n",
        "\t\tr = random.uniform(0, 1)\n",
        "\t\tif r > p:\n",
        "\t\t\tnew_words.append(word)\n",
        "\n",
        "\t#if you end up deleting all words, just return a random word\n",
        "\tif len(new_words) == 0:\n",
        "\t\trand_int = random.randint(0, len(words)-1)\n",
        "\t\treturn [words[rand_int]]\n",
        "\n",
        "\treturn new_words\n",
        "\n",
        "########################################################################\n",
        "# Random swap\n",
        "# Randomly swap two words in the sentence n times\n",
        "########################################################################\n",
        "\n",
        "def random_swap(words, n):\n",
        "\tnew_words = words.copy()\n",
        "\tfor _ in range(n):\n",
        "\t\tnew_words = swap_word(new_words)\n",
        "\treturn new_words\n",
        "\n",
        "def swap_word(new_words):\n",
        "\trandom_idx_1 = random.randint(0, len(new_words)-1)\n",
        "\trandom_idx_2 = random_idx_1\n",
        "\tcounter = 0\n",
        "\twhile random_idx_2 == random_idx_1:\n",
        "\t\trandom_idx_2 = random.randint(0, len(new_words)-1)\n",
        "\t\tcounter += 1\n",
        "\t\tif counter > 3:\n",
        "\t\t\treturn new_words\n",
        "\tnew_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1] \n",
        "\treturn new_words\n",
        "\n",
        "########################################################################\n",
        "# Random insertion\n",
        "# Randomly insert n words into the sentence\n",
        "########################################################################\n",
        "\n",
        "def random_insertion(words, n):\n",
        "\tnew_words = words.copy()\n",
        "\tfor _ in range(n):\n",
        "\t\tadd_word(new_words)\n",
        "\treturn new_words\n",
        "\n",
        "def add_word(new_words):\n",
        "\tsynonyms = []\n",
        "\tcounter = 0\n",
        "\twhile len(synonyms) < 1:\n",
        "\t\trandom_word = new_words[random.randint(0, len(new_words)-1)]\n",
        "\t\tsynonyms = get_synonyms(random_word)\n",
        "\t\tcounter += 1\n",
        "\t\tif counter >= 10:\n",
        "\t\t\treturn\n",
        "\trandom_synonym = synonyms[0]\n",
        "\trandom_idx = random.randint(0, len(new_words)-1)\n",
        "\tnew_words.insert(random_idx, random_synonym)\n",
        "\n",
        "########################################################################\n",
        "# main data augmentation function\n",
        "########################################################################\n",
        "\n",
        "def eda(sentence, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, p_rd=0.1, num_aug=9):\n",
        "\t\n",
        "\tsentence = get_only_chars(sentence)\n",
        "\twords = sentence.split(' ')\n",
        "\twords = [word for word in words if word is not '']\n",
        "\tnum_words = len(words)\n",
        "\t\n",
        "\taugmented_sentences = []\n",
        "\tnum_new_per_technique = int(num_aug/4)+1\n",
        "\tn_sr = max(1, int(alpha_sr*num_words))\n",
        "\tn_ri = max(1, int(alpha_ri*num_words))\n",
        "\tn_rs = max(1, int(alpha_rs*num_words))\n",
        "\n",
        "\t#sr\n",
        "\tfor _ in range(num_new_per_technique):\n",
        "\t\ta_words = synonym_replacement(words, n_sr)\n",
        "\t\taugmented_sentences.append(' '.join(a_words))\n",
        "\n",
        "\t#ri\n",
        "\tfor _ in range(num_new_per_technique):\n",
        "\t\ta_words = random_insertion(words, n_ri)\n",
        "\t\taugmented_sentences.append(' '.join(a_words))\n",
        "\n",
        "\t#rs\n",
        "\tfor _ in range(num_new_per_technique):\n",
        "\t\ta_words = random_swap(words, n_rs)\n",
        "\t\taugmented_sentences.append(' '.join(a_words))\n",
        "\n",
        "\t#rd\n",
        "\tfor _ in range(num_new_per_technique):\n",
        "\t\ta_words = random_deletion(words, p_rd)\n",
        "\t\taugmented_sentences.append(' '.join(a_words))\n",
        "\n",
        "\taugmented_sentences = [get_only_chars(sentence) for sentence in augmented_sentences]\n",
        "\tshuffle(augmented_sentences)\n",
        "\n",
        "\t#trim so that we have the desired number of augmented sentences\n",
        "\tif num_aug >= 1:\n",
        "\t\taugmented_sentences = augmented_sentences[:num_aug]\n",
        "\telse:\n",
        "\t\tkeep_prob = num_aug / len(augmented_sentences)\n",
        "\t\taugmented_sentences = [s for s in augmented_sentences if random.uniform(0, 1) < keep_prob]\n",
        "\n",
        "\t#append the original sentence\n",
        "\taugmented_sentences.append(sentence)\n",
        "\n",
        "\treturn augmented_sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EG9ItHZpnOyv",
        "colab_type": "text"
      },
      "source": [
        "**This module if for appending the generated dateset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GtAeV3et4jI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Rtext,Rtitle,topics=[],[],[]\n",
        "for sent1,sent2,top in zip(RevText_train,RevTitle_train,topic):\n",
        "  sentence1=eda(sent1)\n",
        "  if len(sent2.split())>2:\n",
        "    sentence2=eda(sent2)\n",
        "  else:\n",
        "    for i in range(10):\n",
        "      \n",
        "      sentence2.append(sent2)\n",
        "  for senten1,senten2 in zip(sentence1,sentence2):\n",
        "    Rtext.append(senten1)\n",
        "    Rtitle.append(senten2)\n",
        "    topics.append(top)\n",
        "    count=count+1\n",
        "\n",
        "print(len(Rtext))\n",
        "print(len(Rtitle))\n",
        "print(len(topics))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lk1Q6nYAng_T",
        "colab_type": "text"
      },
      "source": [
        "Creating a DataFrame of the Data Generated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ch48YzvXTYsU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del train\n",
        "train=pd.DataFrame(columns=['revtext','revtitle'])\n",
        "train['revtext']=Rtext\n",
        "train['revtitle']=Rtitle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HN0NSr6zSifo",
        "colab_type": "code",
        "outputId": "ddc7acb0-268f-4b7c-948b-c0ac6d32926f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "train.head(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>revtext</th>\n",
              "      <th>revtitle</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>did nothing for me didnt help lost even with w...</td>\n",
              "      <td>they horrendous smell awful</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>did nothing for me didnt help lost even with w...</td>\n",
              "      <td>they smell awful</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>did nothing for me didnt help lost even with w...</td>\n",
              "      <td>they smell awful</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>did nothing for me didnt help lost even with w...</td>\n",
              "      <td>they awful smell</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>did nothing for me didnt help lost and with wo...</td>\n",
              "      <td>they awful smell</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             revtext                     revtitle\n",
              "0  did nothing for me didnt help lost even with w...  they horrendous smell awful\n",
              "1  did nothing for me didnt help lost even with w...             they smell awful\n",
              "2  did nothing for me didnt help lost even with w...             they smell awful\n",
              "3  did nothing for me didnt help lost even with w...             they awful smell\n",
              "4  did nothing for me didnt help lost and with wo...             they awful smell"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ECFl29MnoqH",
        "colab_type": "text"
      },
      "source": [
        "Label Encoding the topic for the complete dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpXGLN2pOnW0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label=labencod.transform(topics)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkPPCay4O0Qo",
        "colab_type": "code",
        "outputId": "5d259842-8783-4f40-f593-d9170a0b7dc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "label.shape"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(29795,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzhzJ0T3nyR2",
        "colab_type": "text"
      },
      "source": [
        "Spliting the dataset into training and validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rh05C4pOO3Cs",
        "colab_type": "code",
        "outputId": "e29e4ec6-dc37-40c4-b863-944e734dd638",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train,val,label_train,label_val=train_test_split(train,label,test_size=0.1)\n",
        "train.shape,val.shape,label_train.shape,label_val.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((26815, 2), (2980, 2), (26815,), (2980,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_F9Uwo5Pdi6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RevText_train=train['revtext']\n",
        "RevTitle_train=train['revtitle']\n",
        "RevText_val=val['revtext']\n",
        "RevTitle_val=val['revtitle']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9110X2oTWB_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lemmetizer=WordNetLemmatizer()\n",
        "stemmer=PorterStemmer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lhy08m5Cn6kM",
        "colab_type": "text"
      },
      "source": [
        "Function for stemming the texts present"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_6iClYwWjL7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stemming(data):\n",
        "  Text=[]\n",
        "  \n",
        "  for sent in data:\n",
        "    desc=\"\"\n",
        "    for word in sent.split():\n",
        "      if word.isalpha():\n",
        "        desc+=stemmer.stem(word)+\" \"\n",
        "    Text.append(desc)\n",
        "  return Text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7m8OROgb7sT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RevText_train=stemming(RevText_train)\n",
        "RevTitle_train=stemming(RevTitle_train)\n",
        "RevText_test=stemming(RevText_test)\n",
        "RevTitle_test=stemming(RevTitle_test)\n",
        "RevText_val=stemming(RevText_val)\n",
        "RevTitle_val=stemming(RevTitle_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zH2lYsFTW3yO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "29ecba26-50aa-4d64-a873-03a59a5f4c75"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EV2U66MYoCdv",
        "colab_type": "text"
      },
      "source": [
        "Funtion for tokenzing the dataset for the complete the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENlC5RyLXtIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenizing(Text,num_of_words):\n",
        "  \n",
        "  tokens=Tokenizer(num_words=num_of_words)\n",
        "  tokens.fit_on_texts(Text)\n",
        "  vocab_size=len(tokens.word_index)+1\n",
        "  texts=tokens.texts_to_sequences(Text)\n",
        "  return vocab_size,texts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTn1CiiWYK7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RevText_train_vocab_size,RevText_train=tokenizing(RevText_train,8000)\n",
        "RevTitle_train_vocab_size,RevTitle_train=tokenizing(RevTitle_train,8000)\n",
        "RevTitle_test_vocab_size,RevTitle_test=tokenizing(RevTitle_test,8000)\n",
        "RevText_test_vocab_size,RevText_test=tokenizing(RevText_test,8000)\n",
        "RevTitle_val_vocab_size,RevTitle_val=tokenizing(RevTitle_val,8000)\n",
        "RevText_val_vocab_size,RevText_val=tokenizing(RevText_val,8000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LVwh9AuYMHK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0c6ecc42-4eed-4bf7-f08d-f44b89e903c8"
      },
      "source": [
        "RevText_train[2],label.shape"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([436, 603, 23, 10, 554, 3, 44, 193, 83, 499], (29795,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQzhJNLVoMBl",
        "colab_type": "text"
      },
      "source": [
        "Padding the tokenized dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ovMu6gpmzvu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Pad_func(Text1,Text2,Text3):\n",
        "  maxlen=np.max([len(t) for t in Text1])\n",
        "  text1=[]\n",
        "  text2=[]\n",
        "  text3=[]\n",
        "  for txt1 in Text1:\n",
        "    \n",
        "    in_txt1=pad_sequences([txt1],maxlen=maxlen,padding='post')\n",
        "    text1.append(in_txt1)\n",
        "  for txt2 in Text2:\n",
        "    in_txt2=pad_sequences([txt2],maxlen=maxlen,padding='post')\n",
        "    text2.append(in_txt2)\n",
        "  for txt3 in Text3:\n",
        "    in_txt3=pad_sequences([txt3],maxlen=maxlen,padding='post')\n",
        "    text3.append(in_txt3)\n",
        "    \n",
        "  return maxlen,text1,text2,text3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01JOqQaZqxWY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maxlen_RevText,RevText_train,RevText_test,RevText_val=Pad_func(RevText_train,RevText_test,RevText_val)\n",
        "maxlen_RevTitle,RevTitle_train,RevTitle_test,RevTitle_val=Pad_func(RevTitle_train,RevTitle_test,RevTitle_val)\n",
        "#maxlen_topic,topic=Pad_func(topic)\n",
        "label=to_categorical(label_train)\n",
        "label_val=to_categorical(label_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KT5Mm54q3F7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RevText_train=np.array(RevText_train)\n",
        "RevTitle_train=np.array(RevTitle_train)\n",
        "RevText_test=np.array([RevText_test])\n",
        "RevTitle_test=np.array([RevTitle_test])\n",
        "RevText_val=np.array([RevText_val])\n",
        "RevTitle_val=np.array([RevTitle_val])\n",
        "\n",
        "#topic=np.array(topic)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6JCPMVjpqYy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RevText_train.shape,RevTitle_train.shape,label.shape,RevText_test.shape,RevTitle_test.shape,RevText_val.shape,RevTitle_val.shape,label_val.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJAQinWZoedA",
        "colab_type": "text"
      },
      "source": [
        "Reshaping the dataset into desired form for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0FqMq_Mn3Cr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RevText_train=np.reshape(RevText_train,(RevText_train.shape[0],RevText_train.shape[2]))\n",
        "RevText_test=np.reshape(RevText_test,(RevText_test.shape[1],RevText_test.shape[3]))\n",
        "\n",
        "RevTitle_train=np.reshape(RevTitle_train,(RevTitle_train.shape[0],RevTitle_train.shape[2]))\n",
        "RevTitle_test=np.reshape(RevTitle_test,(RevTitle_test.shape[1],RevTitle_test.shape[3]))\n",
        "\n",
        "RevText_val=np.reshape(RevText_val,(RevText_val.shape[1],RevText_val.shape[3]))\n",
        "RevTitle_val=np.reshape(RevTitle_val,(RevTitle_val.shape[1],RevTitle_val.shape[3]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZirDbRzFt5kA",
        "colab_type": "code",
        "outputId": "3b2f96f9-c6a9-4df6-d79b-18f401b1f347",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "RevText_train.shape,RevTitle_train.shape,label.shape,RevText_test.shape,RevTitle_test.shape,RevText_val.shape,RevTitle_val.shape,label_val.shape"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((26815, 612),\n",
              " (26815, 24),\n",
              " (26815, 21),\n",
              " (2553, 612),\n",
              " (2553, 24),\n",
              " (2980, 612),\n",
              " (2980, 24),\n",
              " (2980, 21))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHCJf-5Xomdc",
        "colab_type": "text"
      },
      "source": [
        "import liberaries for defining the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8Ud8axVxNv6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers\n",
        "from keras.layers import Dense, LSTM, Dropout,Input,Embedding\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam, RMSprop, Adamax"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6urjnxeoso0",
        "colab_type": "text"
      },
      "source": [
        "Creating model for the problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmugpNPpj0fD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "9a4e9465-69be-4c43-c545-c1e4f28a9ab2"
      },
      "source": [
        "emb_dim1=800\n",
        "emb_dim2=50\n",
        "\n",
        "txt_in1=Input(shape=(maxlen_RevText,))\n",
        "text_1=Embedding(RevText_train_vocab_size,emb_dim1,mask_zero=True)(txt_in1)\n",
        "lstm_1=LSTM(128,name=\"Rev_Text\")(text_1)\n",
        "\n",
        "txt_in2=Input(shape=(maxlen_RevTitle,))\n",
        "text_2=Embedding(RevTitle_train_vocab_size,emb_dim2,mask_zero=True)(txt_in2)\n",
        "lstm_2=LSTM(128,name=\"Rev_Title\")(text_2)\n",
        "\n",
        "Decode_1=layers.add([lstm_1,lstm_2])\n",
        "Decode_1=Dense(256,activation='relu')(Decode_1)\n",
        "Decode_1=Dense(64,activation='relu')(Decode_1)\n",
        "out=Dense(label.shape[1],activation='softmax')(Decode_1)\n",
        "model=Model(inputs=[txt_in1,txt_in2],outputs=out)\n",
        "\n",
        "opt=Adam(lr=0.005, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=True)\n",
        "model.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0811 12:03:45.054455 140490663610240 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0811 12:03:45.091276 140490663610240 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0811 12:03:45.099832 140490663610240 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0811 12:03:45.348283 140490663610240 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2974: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0811 12:03:45.666779 140490663610240 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0811 12:03:45.690147 140490663610240 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxXd19Oko8Cp",
        "colab_type": "text"
      },
      "source": [
        "Training model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Shh832sAlcER",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit([RevText_train,RevTitle_train],label,epochs=15,verbose=1,batch_size=512,validation_data=([RevText_val,RevTitle_val],label_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E996-Wejo-iM",
        "colab_type": "text"
      },
      "source": [
        "Predicting Results for our test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcSfP2sjmNu4",
        "colab_type": "code",
        "outputId": "2a8e9162-6f92-471f-d4e7-10dbe0d88b34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "result=model.predict([RevText_test,RevTitle_test],verbose=1)\n",
        "#print(result_dict[model.predict([RevText_test,RevTitle_test],verbose=1)])"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2553/2553 [==============================] - 34s 13ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvFgB4Kr6RBx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "values=np.argmax(result,axis=1)\n",
        "appending=[]\n",
        "for i in values:\n",
        "  appending.append(result_dict[i])\n",
        "  #print(test)\n",
        "  #j=j+1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cK8Uk3y7odW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test['topic']=appending\n",
        "test.head(20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATzt8ttL774h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test.to_csv('Submission.csv',index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zp0MrEp7pjY4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}